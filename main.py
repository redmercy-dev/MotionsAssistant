import json, mimetypes, os, tempfile, uuid
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
import hmac

import requests
import pandas as pd
import streamlit as st
from google import genai
from google.genai import types as gtypes
from openai import OpenAI

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STATIC CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CFG_PATH = "config_tender.json"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STREAMLIT HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def uk(prefix: str = "k") -> str:
    """Return a unique Streamlit widget key."""
    return f"{prefix}_{uuid.uuid4().hex}"

def format_citation_text(text: str, max_length: int = 200) -> str:
    """Format citation text for display, truncating if needed."""
    if not text:
        return ""
    if len(text) <= max_length:
        return text
    return text[:max_length] + "..."

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PERSISTED CFG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_cfg() -> Dict:
    if not Path(CFG_PATH).exists():
        return {"vector_store_id": None}
    with open(CFG_PATH, "r", encoding="utf-8") as f:
        cfg = json.load(f)
    cfg.setdefault("vector_store_id", None)
    return cfg

def save_cfg(c: Dict):
    with open(CFG_PATH, "w", encoding="utf-8") as f:
        json.dump(c, f, indent=2)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG & API KEYS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cfg = load_cfg()
openai_api_key = st.secrets["api_keys"]["openai_api_key"]
gemini_api_key = st.secrets["api_keys"]["gemini_api_key"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ OPENAI CLIENT (Responses API) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
client = OpenAI(api_key=openai_api_key)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GOOGLE GEMINI CLIENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
g_client = genai.Client(api_key=gemini_api_key)
GEM_MODEL = "gemini-2.0-flash-exp"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Password â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def check_password():
    """
    Returns True if the user enters the correct password stored in Streamlit secrets.
    """
    def password_entered():
        if hmac.compare_digest(str(st.session_state["password"]), str(st.secrets["APP_PASSWORD"])):
            st.session_state["password_correct"] = True
            del st.session_state["password"]
        else:
            st.session_state["password_correct"] = False

    if st.session_state.get("password_correct", False):
        return True

    st.text_input("Mot de passe", type="password", on_change=password_entered, key="password")
    if "password_correct" in st.session_state:
        st.error("ðŸ˜• Mot de passe incorrect")
    return False

if not check_password():
    st.stop()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Gemini Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_mime(path: str) -> str:
    m, _ = mimetypes.guess_type(path)
    return m or "application/octet-stream"

def gem_upload(path: str) -> gtypes.File:
    """Upload file to Gemini and return File object."""
    return g_client.files.upload(file=path)

def gem_extract(path: str, filename: str) -> str:
    """
    Uploads path to Gemini and asks it to extract relevant information for tender response.
    """
    gfile = gem_upload(path)

    prompt = f"""Tu es un assistant spÃ©cialisÃ© dans l'analyse de documents d'appels d'offres franÃ§ais.

Analyse ce document ({filename}) et extrais TOUTES les informations structurÃ©es pertinentes:

1. Si c'est un RC (RÃ¨glement de Consultation):
   - Type de marchÃ© et objet
   - CritÃ¨res d'Ã©valuation avec pondÃ©ration
   - Documents obligatoires Ã  fournir
   - Planning demandÃ© (OUI/NON)
   - CVs demandÃ©s (OUI/NON, prÃ©cise les postes)
   - Contraintes spÃ©cifiques (site occupÃ©, phasage, dÃ©lais, etc.)
   - Structure attendue du MÃ©moire Technique

2. Si c'est un CCAP/CCTP:
   - Exigences techniques clÃ©s
   - Normes et certifications requises
   - ModalitÃ©s d'exÃ©cution
   - Points de vigilance

3. Si c'est un document de rÃ©fÃ©rence (exemple, proposition concurrente):
   - Points forts identifiÃ©s
   - Structure utilisÃ©e
   - Arguments mis en avant

RÃ©ponds en JSON structurÃ© ou en texte clair avec des sections bien dÃ©finies.
Si le document n'est pas pertinent, rÃ©ponds: NO_RELEVANT_INFO"""

    contents = [
        gtypes.Content(
            role="user",
            parts=[
                gtypes.Part.from_text(text=prompt),
                gtypes.Part.from_uri(file_uri=gfile.uri, mime_type=gfile.mime_type),
            ],
        ),
    ]
    resp = g_client.models.generate_content(
        model=GEM_MODEL,
        contents=contents,
    )
    return (resp.text or "").strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SYSTEM INSTRUCTIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYSTEM_INSTRUCTIONS = """Role & Goal
Tu es un assistant IA aidant Ã  gÃ©nÃ©rer des livrables de rÃ©ponse Ã  des appels d'offres franÃ§ais ("MÃ©moire Technique", "Planning" optionnel, "CVs" optionnels, analyse concurrentielle optionnelle) Ã  partir de documents d'appel d'offres tÃ©lÃ©chargÃ©s par l'utilisateur (RC requis; CCAP/CCTP optionnels) et de documents de rÃ©fÃ©rence internes (exemples SEF/templates, propositions passÃ©es, etc.).

RÃ¨gle fondamentale: Automatisation pilotÃ©e par le RC
- Le RC est la source de vÃ©ritÃ© pour ce qui doit Ãªtre gÃ©nÃ©rÃ© et comment cela doit Ãªtre structurÃ©.
- Tu dois automatiquement infÃ©rer les sections requises et les livrables conditionnels (planning, CVs) Ã  partir du contenu du RC.
- L'utilisateur ne devrait PAS avoir besoin de dÃ©finir manuellement la structure.

Politique RAG-first et anti-hallucination
- Consulte TOUJOURS les passages rÃ©cupÃ©rÃ©s de la base de connaissances / documents tÃ©lÃ©chargÃ©s avant de faire des affirmations factuelles.
- N'invente JAMAIS de noms, certifications, contraintes de projet, quantitÃ©s, dÃ©lais ou exigences client.
- Si un dÃ©tail requis est manquant, indique clairement ce qui manque et propose un format de placeholder (ex: "[Ã€ complÃ©ter: â€¦]") et demande Ã  l'utilisateur/admin de fournir l'information manquante.

Sorties supportÃ©es
Selon la "tÃ¢che" demandÃ©e par l'utilisateur, tu dois produire:
A) RC_INTERPRETATION_JSON
B) MEMOIRE_TECHNIQUE_DRAFT
C) PLANNING_SPEC (tÃ¢ches de haut niveau + durÃ©es + dÃ©pendances)
D) CV_SPEC (donnÃ©es CV structurÃ©es par poste)
E) COMPETITOR_ANALYSIS_REPORT

Style de sortie
- Langue franÃ§aise, ton professionnel d'appel d'offres.
- Titres clairs, points bullet, sections structurÃ©es.
- Alignement explicite aux critÃ¨res d'Ã©valuation (si prÃ©sents): les critÃ¨res Ã  fort poids doivent recevoir plus de dÃ©tails.
- Ã‰vite le marketing vague; privilÃ©gie mÃ©thodologie concrÃ¨te, organisation, contraintes, livrables, QA/HSE, et phasage.

A) Format RC_INTERPRETATION_JSON (strict)
Retourne un objet JSON avec:
{
  "business_type": "new_construction|renovation|maintenance|unknown",
  "evaluation_criteria": [{"name": "...", "weight": "..."}, ...],
  "mandatory_documents": ["...", ...],
  "planning_required": true|false,
  "cvs_required": true|false,
  "required_roles_for_cvs": ["...", ...],
  "required_sections": [{"title":"...", "purpose":"...", "key_points":["..."]}],
  "special_constraints": ["...", ...],
  "open_questions": ["...", ...]
}

B) Format MEMOIRE_TECHNIQUE_DRAFT
Retourne:
- Titre
- Table des matiÃ¨res (titres de sections)
- Puis chaque section avec:
  - Objectif de la section
  - Contenu (franÃ§ais, professionnel)
  - Bullets "ConformitÃ© RC" mappant: quelle exigence RC cela adresse (si connu)
- Termine avec une table "Checklist de conformitÃ©" (Exigence â†’ OÃ¹ traitÃ© â†’ Statut: couvert/partiel/manquant)

C) Format PLANNING_SPEC
Retourne un objet JSON avec:
{
  "assumptions": ["..."],
  "calendar": {"work_days": "...", "constraints": ["..."]},
  "tasks": [
    {"id":"T1","name":"...","duration_days":X,"depends_on":["T0"],"notes":"..."},
    ...
  ],
  "milestones": [{"name":"...","day":X}]
}
Garde-le rÃ©aliste et cohÃ©rent avec les contraintes du RC (phasage, site occupÃ©, limites d'accÃ¨s, etc.). Si contraintes manquantes, mets des hypothÃ¨ses explicitement.

D) Format CV_SPEC
Retourne un objet JSON avec:
{
  "roles": [
    {
      "role_name":"...",
      "required_by_rc": true,
      "candidates": [
        {
          "full_name":"(seulement si fourni dans les donnÃ©es; sinon placeholder)",
          "title":"...",
          "years_experience":"...",
          "key_projects":["..."],
          "certifications":["..."],
          "responsibilities_on_this_tender":["..."]
        }
      ]
    }
  ],
  "missing_employee_data": ["..."]
}
N'invente pas de personnes ou de certifications.

E) Format COMPETITOR_ANALYSIS_REPORT
Retourne:
- RÃ©sumÃ© des forces/faiblesses concurrentes (basÃ© uniquement sur les docs concurrents fournis)
- Points de comparaison vs approche SEF
- 10 amÃ©liorations actionnables pour la proposition SEF, alignÃ©es avec les critÃ¨res d'Ã©valuation
- Bullets "Preuves" rÃ©fÃ©renÃ§ant ce qui a Ã©tÃ© vu dans les docs concurrents (pas de citations >25 mots)

Comportement gÃ©nÃ©ral
- Si on te demande de gÃ©nÃ©rer un livrable que le RC n'exige PAS (ex: planning quand le RC ne le demande pas), dis: "Le RC ne semble pas exiger X. Confirmez si vous souhaitez quand mÃªme le gÃ©nÃ©rer."
- Garde toujours les sorties structurÃ©es et prÃªtes pour le rendu de documents en aval (python-docx/reportlab).

IMPORTANT: Utilise TOUJOURS les passages rÃ©cupÃ©rÃ©s via RAG avant de rÃ©pondre. Base tes rÃ©ponses sur les documents fournis."""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ OpenAI Container File Download Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def download_container_file(container_id: str, file_id: str, filename: str) -> Tuple[str, bytes]:
    """
    Download a file from a code interpreter container.
    """
    try:
        url = f"https://api.openai.com/v1/containers/{container_id}/files/{file_id}/content"
        headers = {"Authorization": f"Bearer {openai_api_key}"}
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return (filename, response.content)
    except Exception as e:
        st.warning(f"Impossible de tÃ©lÃ©charger le fichier {filename}: {str(e)}")
        return (filename, b"")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Robust helpers for SDK objects/dicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _get(obj: Any, key: str, default: Any = None) -> Any:
    if obj is None:
        return default
    if isinstance(obj, dict):
        return obj.get(key, default)
    return getattr(obj, key, default)

def _as_list(x: Any) -> List[Any]:
    if x is None:
        return []
    if isinstance(x, list):
        return x
    return [x]

def _content_to_text(content_obj: Any) -> str:
    """
    Convert a 'content' field from file_search results into displayable text.
    """
    if content_obj is None:
        return ""
    if isinstance(content_obj, str):
        return content_obj.strip()

    if isinstance(content_obj, dict):
        t = _get(content_obj, "text")
        if isinstance(t, str) and t.strip():
            return t.strip()
        parts = _get(content_obj, "content") or _get(content_obj, "parts")
        return _content_to_text(parts)

    if isinstance(content_obj, list):
        texts = []
        for item in content_obj:
            if isinstance(item, str):
                if item.strip():
                    texts.append(item.strip())
                continue
            if isinstance(item, dict):
                t = item.get("text") or item.get("content")
                if isinstance(t, str) and t.strip():
                    texts.append(t.strip())
                else:
                    maybe = item.get("text")
                    if isinstance(maybe, str) and maybe.strip():
                        texts.append(maybe.strip())
        return "\n\n".join([t for t in texts if t]).strip()

    return str(content_obj).strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Extract container file outputs + retrieved chunks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_container_files_and_chunks(response_obj) -> Tuple[List[Dict], List[Dict]]:
    """
    Returns:
      - container_files: from 'container_file_citation' annotations (code_interpreter outputs)
      - chunks: from file_search_call.results (actual retrieved passages)
    """
    container_files: List[Dict] = []
    chunks: List[Dict] = []

    output_items = _as_list(_get(response_obj, "output"))

    # 1) Container files (from message annotations)
    for output_item in output_items:
        content_list = _as_list(_get(output_item, "content"))
        for content in content_list:
            annotations = _as_list(_get(content, "annotations"))
            for ann in annotations:
                if _get(ann, "type") == "container_file_citation":
                    container_files.append(
                        {
                            "container_id": _get(ann, "container_id"),
                            "file_id": _get(ann, "file_id"),
                            "filename": _get(ann, "filename"),
                        }
                    )

    # 2) Retrieved chunks (from file_search_call.results)
    for output_item in output_items:
        if _get(output_item, "type") != "file_search_call":
            continue

        results = _get(output_item, "results")
        if results is None:
            results = _get(output_item, "search_results")

        for r in _as_list(results):
            file_id = _get(r, "file_id") or _get(r, "file")
            filename = _get(r, "filename")

            if not filename and file_id:
                try:
                    fobj = client.files.retrieve(file_id)
                    filename = getattr(fobj, "filename", None) or "Fichier inconnu"
                except Exception:
                    filename = "Fichier inconnu"

            text = (
                _get(r, "text")
                or _get(r, "chunk")
                or _content_to_text(_get(r, "content"))
                or _content_to_text(_get(r, "document"))
            )

            chunk = {
                "file_id": file_id,
                "filename": filename or "Fichier inconnu",
                "text": text or "",
                "score": _get(r, "score"),
                "rank": _get(r, "rank"),
            }
            if chunk["text"].strip():
                chunks.append(chunk)

    container_files = [
        fa for fa in container_files
        if fa.get("container_id") and fa.get("file_id") and fa.get("filename")
    ]
    return container_files, chunks

def _retrieve_response_with_include(response_id: str):
    """
    Some SDK versions support include=... on retrieve; others don't.
    Try with include first, then fall back.
    """
    try:
        return client.responses.retrieve(response_id, include=["file_search_call.results"])
    except TypeError:
        return client.responses.retrieve(response_id)
    except Exception:
        return client.responses.retrieve(response_id)

def stream_response_with_file_search(
    conversation_history: List[Dict],
    vector_store_id: str,
    context: str
) -> Tuple[str, List[Tuple[str, bytes]], List[Dict]]:
    """
    Stream a response using Responses API with file_search and code_interpreter tools.

    Returns:
      (response_text, list_of_downloads, retrieved_chunks)
    """
    input_items = [
        {"role": "system", "content": SYSTEM_INSTRUCTIONS},
        {"role": "system", "content": context},
    ]
    input_items.extend(conversation_history)

    try:
        stream_response = client.responses.create(
            model="gpt-4o",
            input=input_items,
            tools=[
                {"type": "file_search", "vector_store_ids": [vector_store_id]},
                {"type": "code_interpreter", "container": {"type": "auto"}},
            ],
            include=["file_search_call.results"],
            stream=True,
        )

        holder = st.empty()
        full_text = ""
        response_id: Optional[str] = None

        for event in stream_response:
            etype = getattr(event, "type", None)
            if etype == "response.output_text.delta":
                delta = getattr(event, "delta", None)
                if delta:
                    full_text += delta
                    holder.markdown(full_text)
            elif etype == "response.created":
                resp = getattr(event, "response", None)
                rid = getattr(resp, "id", None) if resp else None
                if rid:
                    response_id = rid

        files_to_download: List[Tuple[str, bytes]] = []
        retrieved_chunks: List[Dict] = []

        if response_id:
            try:
                complete_response = _retrieve_response_with_include(response_id)
                container_files, retrieved_chunks = extract_container_files_and_chunks(complete_response)

                for ann in container_files:
                    filename, file_bytes = download_container_file(
                        ann["container_id"],
                        ann["file_id"],
                        ann["filename"],
                    )
                    if file_bytes:
                        files_to_download.append((filename, file_bytes))

            except Exception as e:
                st.warning(f"Impossible de rÃ©cupÃ©rer les fichiers/chunks de la rÃ©ponse: {str(e)}")

        return full_text, files_to_download, retrieved_chunks

    except Exception as e:
        st.error(f"Erreur lors de la crÃ©ation de la rÃ©ponse: {str(e)}")
        return "", [], []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STREAMLIT UI STYLES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config("GÃ©nÃ©rateur de RÃ©ponse d'Appel d'Offres", layout="wide")
st.markdown(
    """
    <style>
    html, body, [class*="st-"] {
        font-family: 'Georgia', serif;
        color: #333;
    }
    body {background-color: #f0f2f6;}
    h1, h2, h3 {color: #0d1b4c; font-weight: bold;}

    .material-symbols-rounded,
    .material-symbols-outlined,
    .material-icons,
    span.material-symbols-rounded,
    span.material-symbols-outlined,
    i.material-icons,
    [data-testid^="chatAvatarIcon-"] span,
    [data-testid="stExpander"] summary span {
        font-family: "Material Symbols Rounded" !important;
        font-weight: normal !important;
        font-style: normal !important;
        line-height: 1 !important;
        letter-spacing: normal !important;
        text-transform: none !important;
        display: inline-block !important;
        white-space: nowrap !important;
        direction: ltr !important;
        -webkit-font-feature-settings: "liga" !important;
        -webkit-font-smoothing: antialiased !important;
        font-variation-settings: "FILL" 0, "wght" 400, "GRAD" 0, "opsz" 24 !important;
    }

    [data-testid="stExpander"] summary {
        display: flex !important;
        align-items: center !important;
        gap: 0.35rem !important;
    }
    [data-testid="stExpander"] summary span {
        flex: 0 0 auto !important;
    }

    .block-container {
        background-color: #ffffff;
        border-radius: 10px;
        padding: 2rem 3rem 3rem 3rem;
        box-shadow: 0 4px 12px rgba(0,0,0,0.05);
        max-width: 1200px;
        margin: 1rem auto;
    }

    [data-testid="stSidebar"] {
        background-color: #e1e5f0;
        padding-top: 1.5rem;
    }
    [data-testid="stSidebar"] h1, [data-testid="stSidebar"] h2, [data-testid="stSidebar"] h3,
    [data-testid="stSidebar"] label, [data-testid="stSidebar"] button p {
        color: #0d1b4c;
    }
    [data-testid="stFileUploader"] button {
        padding: 6px 12px;
        font-size: 14px;
        border: 1px solid #198754;
        background-color: #198754;
        color: white;
        border-radius: 6px;
    }
    [data-testid="stFileUploader"] button:hover {
        background-color: #157347;
    }

    [data-testid="stChatInput"] textarea {
        font-size: 16px !important;
        line-height: 1.6 !important;
        padding: 12px 15px !important;
        border-radius: 8px !important;
        border: 1px solid #ccc;
        background-color: #f8f9fa;
    }
    [data-testid="stChatInput"] textarea:focus {
        border-color: #0d1b4c;
        box-shadow: 0 0 0 2px rgba(13, 27, 76, 0.2);
    }

    .stChatMessage {
        border-radius: 10px;
        padding: 1rem 1.5rem;
        margin-bottom: 1rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }

    [data-testid="stChatMessageContent"] {
        background-color: transparent;
    }

    div[data-testid="stChatMessage"]:has([data-testid="chatAvatarIcon-assistant"]) {
        background-color: #e1e5f0 !important;
        border-left: 4px solid #0d1b4c;
    }

    div[data-testid="stChatMessage"]:has([data-testid="chatAvatarIcon-user"]) {
        background-color: #d1e7dd !important;
        border-right: 4px solid #198754;
    }

    [data-testid="chatAvatarIcon-user"],
    [data-testid="chatAvatarIcon-assistant"] {
        background-color: transparent !important;
    }

    .stButton button {
        background-color: #198754;
        color: white;
        border: none;
        border-radius: 6px;
        padding: 0.6rem 1.2rem;
        font-weight: bold;
    }
    .stButton button:hover:not(:disabled) {background-color: #157347;}
    .stButton button:disabled {background-color: #cccccc; color: #888888;}
    .stDownloadButton button {
        background-color: #5c6ac4;
        color: white;
        border: none;
        border-radius: 5px;
        padding: 0.3rem 0.8rem;
        font-size: 14px;
        margin-top: 5px;
        margin-right: 5px;
    }
    .stDownloadButton button:hover:not(:disabled) {background-color: #4553a0;}
    </style>
    """,
    unsafe_allow_html=True,
)

page = st.sidebar.radio("Page", ("Chat", "Admin"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RESET BUTTON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.sidebar.button("ðŸ”„ RÃ©initialiser l'espace"):
    Path(CFG_PATH).unlink(missing_ok=True)
    cfg = {"vector_store_id": None}
    st.session_state.clear()
    st.sidebar.success("Espace effacÃ© â€“ ouvrez *Admin* pour recommencer.")
    st.rerun()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLEAR CHAT BUTTON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if page == "Chat" and st.sidebar.button("ðŸ—‘ï¸ Effacer le chat"):
    st.session_state.history = []
    st.sidebar.success("Historique du chat effacÃ©.")
    st.rerun()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SESSION INIT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "history" not in st.session_state:
    st.session_state.history: List[Dict] = []
if "uploader_key" not in st.session_state:
    st.session_state.uploader_key = uk("chat_files")

# ============================================================================
# ADMIN
# ============================================================================
if page == "Admin":
    st.title("âš™ï¸ Panneau d'administration")

    # âžŠ Create vector store
    if not cfg.get("vector_store_id") and st.button("CrÃ©er le vector store"):
        vs = client.vector_stores.create(name="tender_documents_store")
        cfg["vector_store_id"] = vs.id
        save_cfg(cfg)
        st.success(f"Vector store crÃ©Ã©: {vs.id}")
        st.rerun()

    # âž‹ Show vector store
    if cfg.get("vector_store_id"):
        st.subheader("Vector store")
        st.markdown(f"**ID:** `{cfg['vector_store_id']}`")

    # âžŒ Upload documents into vector store
    if cfg.get("vector_store_id"):
        st.subheader("TÃ©lÃ©charger des documents de rÃ©fÃ©rence")
        with st.form("upload_form", clear_on_submit=True):
            doc_type = st.selectbox(
                "Type de document",
                ["RC", "CCAP", "CCTP", "Exemple SEF", "Template", "Proposition concurrente", "Autre"]
            )
            files_ = st.file_uploader("Fichiers PDF", type="pdf", accept_multiple_files=True)
            submitted = st.form_submit_button("TÃ©lÃ©charger")

            if submitted:
                if not files_:
                    st.error("SÃ©lectionnez au moins un fichier PDF.")
                else:
                    with st.spinner("TÃ©lÃ©chargement et indexation â€¦"):
                        for f in files_:
                            file_obj = client.files.create(file=f, purpose="assistants")
                            client.vector_stores.files.create(
                                vector_store_id=cfg["vector_store_id"],
                                file_id=file_obj.id
                            )
                    st.success(f"{len(files_)} fichier(s) tÃ©lÃ©chargÃ©(s) et indexÃ©(s).")
                    st.rerun()

    # âž Display indexed documents
    if cfg.get("vector_store_id"):
        st.subheader("Documents indexÃ©s")
        try:
            vs_files = client.vector_stores.files.list(vector_store_id=cfg["vector_store_id"], limit=100)
            items = getattr(vs_files, "data", None) or vs_files
            
            rows = []
            for vf in items:
                file_id = getattr(vf, "file_id", None) or getattr(vf, "id", None)
                fname = "(inconnu)"
                if file_id:
                    try:
                        file_obj = client.files.retrieve(file_id)
                        fname = getattr(file_obj, "filename", None) or fname
                    except Exception:
                        pass
                rows.append({
                    "Fichier": fname,
                    "ID": file_id or "N/A",
                })
            
            if rows:
                st.dataframe(pd.DataFrame(rows), use_container_width=True)
            else:
                st.info("Aucun document indexÃ© pour le moment.")
        except Exception as e:
            st.warning(f"Impossible de lister les fichiers: {e}")

# ============================================================================
# CHAT
# ============================================================================
if page == "Chat":
    st.title("ðŸ“‹ GÃ©nÃ©rateur de RÃ©ponse d'Appel d'Offres")

    if not cfg.get("vector_store_id"):
        st.error("Vector store non configurÃ©. Veuillez aller dans Admin pour le crÃ©er.")
        st.stop()

    # â”€â”€â”€â”€â”€ Task selector â”€â”€â”€â”€â”€
    task_type = st.sidebar.selectbox(
        "Type de sortie",
        [
            "ðŸ’¬ Discussion libre",
            "ðŸ” InterprÃ©ter le RC",
            "ðŸ“„ GÃ©nÃ©rer MÃ©moire Technique",
            "ðŸ“… GÃ©nÃ©rer Planning",
            "ðŸ‘¤ GÃ©nÃ©rer CVs",
            "ðŸ”Ž Analyser concurrence"
        ],
        key="task_type",
    )

    version = st.sidebar.selectbox("Version", ["V1", "V2", "V3"], key="version")

    # â”€â”€â”€â”€â”€ DISPLAY HISTORY â”€â”€â”€â”€â”€
    for h in st.session_state.history:
        avatar = "ðŸ™‚" if h["role"] == "user" else "ðŸ¤–"
        with st.chat_message(h["role"], avatar=avatar):
            st.markdown(h["content"])

            for fn, blob in h.get("files", []):
                st.download_button(f"TÃ©lÃ©charger {fn}", blob, fn, key=uk("dl_hist"))

            chunks = h.get("citations", [])
            if chunks and h["role"] == "assistant":
                with st.expander(f"ðŸ“š Voir {len(chunks)} passage(s) rÃ©cupÃ©rÃ©(s)", expanded=False):
                    for idx, c in enumerate(chunks, 1):
                        fname = c.get("filename", "Fichier inconnu")
                        score = c.get("score")
                        header = f"**Passage {idx}** (de {fname})"
                        if score is not None:
                            header += f" â€” score: `{score}`"
                        st.markdown(header)

                        txt = c.get("text", "")
                        if txt:
                            st.markdown(f"> {format_citation_text(txt, max_length=800)}")
                        st.divider()

    st.markdown("<div style='padding-bottom:70px'></div>", unsafe_allow_html=True)

    # â”€â”€â”€â”€â”€ Upload + Chat input widgets â”€â”€â”€â”€â”€
    col_inp, col_up = st.columns([5, 2])
    with col_up:
        uploaded = st.file_uploader(
            "ðŸ“Ž Documents",
            type=["pdf", "docx", "txt"],
            accept_multiple_files=True,
            key=st.session_state.uploader_key,
            label_visibility="collapsed"
        )

    with col_inp:
        user_prompt = st.chat_input("Posez votre question ou continuez â€¦")

    # â”€â”€â”€â”€â”€ Handle new turn â”€â”€â”€â”€â”€
    if user_prompt:
        extract_blocks, blobs_for_history = [], []

        if uploaded:
            prog = st.progress(0.0)
            for i, uf in enumerate(uploaded, 1):
                with tempfile.NamedTemporaryFile(delete=False, suffix=f"_{uf.name}") as tmp:
                    tmp.write(uf.getvalue())
                    tmp_path = tmp.name

                with st.spinner(f"Gemini analyse {uf.name} â€¦"):
                    gem_text = gem_extract(tmp_path, uf.name)
                    if gem_text and gem_text not in ("NO_RELEVANT_INFO", "NO_RELEVANT_INFO_FOUND_IN_UPLOAD"):
                        extract_blocks.append(f"EXTRACTED_FROM_UPLOAD Nom du fichier ({uf.name}):\n{gem_text}")
                    blobs_for_history.append((uf.name, uf.getvalue()))

                prog.progress(i / len(uploaded))

                try:
                    os.unlink(tmp_path)
                except Exception:
                    pass

            prog.empty()

        # Store user turn
        st.session_state.history.append(
            {"role": "user", "content": user_prompt, "files": blobs_for_history}
        )

        with st.chat_message("user", avatar="ðŸ™‚"):
            st.markdown(user_prompt)
            for fn, blob in blobs_for_history:
                st.download_button(f"TÃ©lÃ©charger {fn}", blob, fn, key=uk("dl_user"))

        # Build context
        context_parts = [
            f"Type de tÃ¢che demandÃ©e: {task_type}",
            f"Version: {version}",
        ]
        if extract_blocks:
            context_parts.append("\n".join(extract_blocks))

        context = "\n".join(context_parts)

        # Prepare conversation for API
        conversation_history = [{"role": hh["role"], "content": hh["content"]} for hh in st.session_state.history]

        # Get response from API
        with st.chat_message("assistant", avatar="ðŸ¤–"):
            answer, new_files, chunks = stream_response_with_file_search(
                conversation_history,
                cfg["vector_store_id"],
                context
            )

            for fn, data in new_files:
                st.download_button(f"TÃ©lÃ©charger {fn}", data, fn, key=uk("dl_asst"))

            if chunks:
                with st.expander(f"ðŸ“š Voir {len(chunks)} passage(s) rÃ©cupÃ©rÃ©(s)", expanded=False):
                    for idx, c in enumerate(chunks, 1):
                        fname = c.get("filename", "Fichier inconnu")
                        score = c.get("score")
                        header = f"**Passage {idx}** (de {fname})"
                        if score is not None:
                            header += f" â€” score: `{score}`"
                        st.markdown(header)

                        txt = c.get("text", "")
                        if txt:
                            st.markdown(f"> {format_citation_text(txt, max_length=800)}")
                        st.divider()

        st.session_state.history.append(
            {"role": "assistant", "content": answer, "files": new_files, "citations": chunks}
        )

        # Reset uploader key â†’ clears file-picker
        st.session_state.uploader_key = uk("chat_files")
        st.rerun()
